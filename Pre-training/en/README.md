## Pre-training on English dataset

**1. How to pre-train a MWP-BERT model:**
```
python en_pretrain.py
```


## Citation

```
@inproceedings{liang2022mwp,
      title={MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving}, 
      author={Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, and Xiangliang Zhang},
      booktitle={Findings of NAACL},
      year={2022}
}
```