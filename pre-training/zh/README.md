## Pre-training on Chinese dataset.

**1. How to pre-train a MWP-BERT model:**
```
python all_pretrain.py
```

**1. How to pre-train a MWP-RoBERTa model:**
```
python all_pretrain_roberta.py
```

## Citation

```
@inproceedings{liang2022mwp,
  title={MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving},
  author={Liang, Zhenwen and Zhang, Jipeng and Wang, Lei and Qin, Wei and Lan, Yunshi and Shao, Jie and Zhang, Xiangliang},
  booktitle={Findings of NAACL 2022},
  pages={997--1009},
  year={2022}
}
```
